\section{Probability}
\begin{frame}{List of keyword for ''probability''}
    \listofkeywords
\end{frame}
\subsection{Assiomi di probabilit\'a}

\begin{frame}{Assiomi probabilit\'a Kolmogorov}
\begin{itemize}
\item La probabilit\'a di un evento \'e $\prob{(E)}\geq0$ per ogni $E\in S$ spazio campionario.
\item La probabilit\'a degli eventi del sample space \'e 1.
\item $\sigma$-additivit\'a: per ogni sequenza numerabile di insiemi disgiunti (eventi mutualmente esclusivi)
\begin{equation*}
\prob{(\cup_{i=1}^{\infty}E_i)}=\sum_{i=1}^{\infty}\prob{(E_i)}
\end{equation*}
\end{itemize}
\begin{block}{Addition rule: probability that A or B will}
\begin{equation*}
\prob{(A\cup B)}=\prob{(A)}+\prob{(B)}-\prob{(A\cap B)}
\end{equation*}
\end{block}
\begin{columns}[T]
\begin{column}{0.3\textwidth}
\begin{block}{Probabilit\'a condizionata}
\[\prob{(B|A)}=\frac{\prob{(B\cap A)}}{\prob{(A)}}
\]
\end{block}
\end{column}
\begin{column}{0.4\textwidth}
\begin{block}{Teorema di Bayes}
\[\prob{(A|B)}=\frac{\prob{(B|A)}\prob{(A)}}{\prob{(B)}}\]
\end{block}
\end{column}
\begin{column}{0.25\textwidth}
\begin{block}{eventi indipendenti}
    \[P(A|B)=P(A)\]
    \end{block}
    \end{column}
\end{columns}
\end{frame}

\begin{wordonframe}{Axioms exercises}
\[\prob{(A\cap B)}=\prob{A}+\prob{B}-\prob{A\cup B}\]
Indipendence: $\prob{A\cap B}=\prob{A}\prob{B}$.
Disjoint: $\prob{A\cap B}=0$.
\end{wordonframe}

\subsection{Variabili casuali (Osservabili), loro statistiche: distribuzioni di probabilit\'a.}

\begin{frame}{Interpretazioni di probabilit\'a}
\begin{block}{Una variabile casuale assume uno specifico valore per ogni elemento dello spazio campionario S.}
RV: \sout{Deterministic} Stochastic phenomena (\keyword{frequentista} Possibili risultati di una misura/\keyword{soggettivista}: elementi del sample space sono ipotesi/proposizioni)
\end{block}
\begin{block}{Probabilit\'a frequentista}
Gli elementi di S sono risultati di una misura, un sottoinsieme A corrisponde a qualsiasi risultato nel sottoinsieme: A \'e detto evento. Se A contiene solo un elemento \'e detto evento elementare: $\prob{(A)}=\lim_{n\to\infty}\frac{\text{number of occurrences of outcome A in n measurements}}{n}$.
\end{block}
\begin{block}{Probabilit\'a bayesiana }
Gli elementi di A sono ipotesi o affermazioni: $\prob{A}$ = Degree of belief that A is true
\begin{itemize}
\item Include relative frequency interpretation: The statement that a measure will yield a given outcome a certain fraction of times can be regarded as a hypothesis.
\item Subjective probability can be associated with value of unknown constants (mass of electron, etc): probability of $95\%$ that electron mass is in given interval.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Distribuzione di probabilit\'a di un osservabile: densit\'a di probabilit\'a, cumulante; osservabili multidimensionali.}

\begin{itemize}
\item Distribuzione di probabilit\'a cumulante: $F(x)=P(x_n<x)$: PDF of X $f(x)=\TDy{x}{F(x)}$.
\item Distribuzione di probabilit\'a di $a(X)$ funzione di variabile casuale X con PDF $f(x)$: $g(a)=f(x(a))|\TDy{a}{x}|$
\item Probabilit\'a $x\in[x,x+dx]$ (A) e $y\in[y,y+dy]$ (B): $\prob{(A\cap B)}=f(x,y)\,dx\,dy$. Marginal pdf for x $f_x(x)=\int f(x,y)\,dy$: $\prob{(A|B)}=\frac{\prob{A\cap B}}{\prob{A}}=\frac{f(x,y)\,dx\,dy}{f_x(x)\,dx}$
\end{itemize}
\end{frame}

\begin{frame}{Momenti di una distribuzione e di una statistica}
%Valore di aspettazione, varianza e momenti di di variabile casuale con data distribuzione di probabilit\'a, di campione (misure) estratto con data pdf e statistiche.

\begin{block}{Valore di aspettazione}
\begin{align*}
&E[X]=(\sum x_iP(X=x_i))=\int f(x)x\,dx\\
&E[a(X)]=\intsinf{}a(x)f(x)\,dx=\intsinf{}ag(a)d\,a
\end{align*}
\end{block}
\begin{columns}[T]
\begin{column}{0.6\textwidth}
\begin{block}{Varianza}
\begin{align*}
&\var{(x_n)}=E[(x_n-\mu)^2]=\mu_2=\sigma^2(X)
\end{align*}
\end{block}
\end{column}
\begin{column}{0.4\textwidth}
\begin{block}{Momenti successivi}
\begin{align*}
&\mu_l=E[(X-\mu)^l]
\end{align*}
\end{block}
\end{column}
\end{columns}
\begin{block}{Momenti di una statistica S}
\begin{align*}
    &\E{(S(\vec{x})-\E{[S]})^l}
\end{align*}
\end{block}
\end{frame}

\begin{wordonframe}{(contro)Esempi momenti}
\begin{align*}
&\mu_n(x_0)=\E{[\sum_k^n]}\binom{n}{k}(-1)^{n-k}\mu'_kx_0^{n-k}
\end{align*}
\begin{block}{pdf di \keyword{Cauchy}: non vale LLN}
\begin{columns}[T]
\begin{column}{0.5\textwidth}
\begin{align*}
&P(x)=\frac{1}{\pi}\frac{1}{1+x^2}\\
&\mu_1=\frac{1}{\pi}\intsinf{}\frac{x}{1+x^2}=\NAN{}
\end{align*}
\end{column}
\begin{column}{0.5\textwidth}
\begin{align*}
&F(x)=\intsinf{}\frac{1}{\pi}\frac{1}{1+x^2}d\,x=\frac{\arctan{x}}{\pi}+\frac{1}{2}
\end{align*}
\end{column}
\end{columns}
\end{block}
\end{wordonframe}

\begin{frame}{Propagazione errori}
    \begin{block}{Propagazione errori}
\begin{align*}
&S(\vec{x})\approx y(\vec{\mu})+\left.\sum_i^n\PDy{x_i}{S}\right|_{\vec{\mu}}(x_i-\mu_i)+o(|\vec{x}-\vec{\mu}|)\\
&\var{(S)}=\E{[(S(\vec{x})-\E{[S]})^2]}\approx\sum_{ij}\PDyat{x_i}{S}{\vec{\mu}}\PDyat{x_i}{y}{\vec{\mu}}\cov{x_ix_j}
\end{align*}
\end{block}
\begin{block}{Covarianza e correlazione}
\begin{columns}[T]
\begin{column}{0.5\textwidth}
\begin{align*}
    \cov{x_ix_j}=\E{[(x-\mu_x)(y-\mu_y)]}=\E{[xy]}-\mu_x\mu_y
\end{align*}
\end{column}
\begin{column}{0.5\textwidth}
\begin{align*}
\rho_{xy}=\frac{\cov{x_ix_j}}{\sigma_i\sigma_j}
\end{align*}
\end{column}
\end{columns}
X,Y indipendenti: $\rho_{XY}=0$ ma non v.v.
\end{block}
\end{frame}

\subsection{Funzione caratteristica}

\begin{frame}{Funzione generatrice dei momenti - Funzione caratteristica}
\begin{columns}[T]
\begin{column}{0.5\textwidth}
\begin{align*}
&M_X(t)=\E{[\exp{tX}]}\\
&\phi_X(t)=\E{[\exp{itX}]}=\int\exp{itx}\,d\mu_X(x)
\end{align*}
\begin{block}{Somma RV (indipendenti))}
\begin{align*}
&X=X_1+X_2\\
&M_X(t)=M_{X_1}M_{X_2}
\end{align*}
\end{block}
\end{column}
\begin{column}{0.5\textwidth}
\begin{align*}
&\PDyn{t}{M_X}{n}|_{t=0}=\E{[x^n]}=\mu_n'\\
&\PDyn{it}{\phi_X}{n}|_{t=0}=\left.(-i)^n\E{[x^n]}\right|_{t=0}=\mu_n'
\end{align*}
\begin{block}{scalare}
\begin{align*}
&Y=\alpha X\\
&M_{\alpha X}=M_X(\alpha t)
\end{align*}
\end{block}
\end{column}
\end{columns}
\end{frame}

\begin{wordonframe}{FGM pdf uniforme}
\begin{columns}[T]
\begin{column}{0.5\textwidth}
\begin{align*}
&p(x)=\frac{1}{m}\ 0<x<m\ (\int_0^mp(x)d\,x=1)\\
&\E{[\exp{tx}]}=\int_0^m\frac{\exp{tx}}{m}d\,x=\frac{\exp{mt}-1}{mt}\\
&=\frac{1}{m}[m+\frac{m^2t}{2}+\frac{m^3t^2}{6}+\frac{m^4t^3}{4!}+\ldots]
\end{align*}
\end{column}
\begin{column}{0.5\textwidth}
\begin{align*}
&\mu'_2=\frac{m^2}{3}=\sigma^2+\mu^2\\
&\sigma^2=\mu_2'-(\frac{m^2}{2})^2=\frac{m^2}{12}
\end{align*}
\end{column}
\end{columns}
\begin{block}{Somma RV con pdf uniforme fra 0 e 1}
$y=x_1+\ldots+x_n$: $\MGF_y=(\frac{\exp{mt}-1}{mt})^n$
\end{block}
\end{wordonframe}

\begin{wordonframe}{PDF distanza stelle distribuite uniformemente in x,y,z}
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{block}{Radial Fourier transform}
\begin{align*}
&\hat{f}(\vec{k})=\int\exp{-i\scap{k}{x}}f(\vec{x})d^nx\\
&=s^{\frac{n-2}{2}}\hat{F}_n(s)\\
&=(2\pi)^{\frac{n}{2}}\int_0^{+\infty}J_{\frac{n-2}{2}}(sr)r^{\frac{n-2}{2}}F(r)r\,dr
\end{align*}
\end{block}
\end{column}
\begin{column}{0.5\textwidth}
\begin{block}{MGF of sum of RV is product of their MGF}
\begin{align*}
&\MGF{x_i^2}=\E{[\exp{tx_i^2}]}\\
&=\frac{\sqrt{\pi}[\Erfi{(b\sqrt{t})}-\Erfi{a\sqrt{t}}]}{2(b-a)\sqrt{t}}
\end{align*}
\end{block}
\end{column}
\end{columns}
\begin{block}{$(x,y,z)$ distribuite Unif.: $(r,\theta,\phi)$??}

\end{block}
\end{wordonframe}

\begin{frame}{Teorema di Paul-Levy}
    
\begin{columns}[T]
\begin{column}{0.3\textwidth}
    Se conosco tutti i momenti di una distribuzione conosco la distribuzione?
\end{column}
\begin{column}{0.7\textwidth}
\begin{itemize}
    \item $F_n$ successione di funzioni cumulanti
    \item $\phi_n\to\phi$ per ogni $t\in I(0)$
    \item $\Re{\phi}$ continua in $I(0)$ (implica pdf \'e FT di $\phi$?)
\end{itemize}
quindi $F_n(x)\to F(x)$.
\end{column}
\end{columns}
\end{frame}

\begin{wordonframe}{Propriet\'a Trasformata di Fourier}
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{align*}
f(x)\FT{}\begin{matrix}
\hat{f}(\omega)=\frac{1}{\sqrt{2\pi}}\intsinf\,f(x)\exp{-i\omega x}\,dx\\
\hat{f}(\nu)=\intsinf\,f(x)\exp{-i\nu x}\,dx\\
\end{matrix}
\end{align*}
\end{column}

\begin{column}{0.5\textwidth}
\begin{align*}
&a*f(x)+bg(x)\FT{}a\hat{f}(\nu)+b\hat{g}(\nu)\\
&h(x)=\exp{2\pi ix\nu_0}f(x)\FT{}\hat{f}(\nu-\nu_0)
\end{align*}
\end{column}
\end{columns}

\begin{columns}
\begin{column}{0.5\textwidth}
Scalar: \[f(ax)\FT{}\frac{1}{|a|}\hat{f}(\frac{\omega}{a}) \]
Translations:
\[f(x-a)\FT{}\exp{-ia\nu}\hat{f}(\nu)f(x)\exp{iax}\]
\[\hat{f}(\xi-\frac{a}{2\pi}) \hat{f}(\nu-a)\]

Derivates:
\[\frac{d^n\,f(x)}{dx^n}\FT{}(i\omega)^n\hat{f}(\omega)\]
\[x^nf(x)\FT{}i^n\frac{d^n\hat{f}(\omega)}{d\omega^n}\]
\end{column}
\begin{column}{0.5\textwidth}
Inverse: \[\hat{a} f(-\omega) 2\pi f(-\nu)\]
Convoluzione e prodotto:
\begin{align*}
&(f*g)(x)\FT{}\sqrt{2\pi}\hat{f}(\omega)\hat{g}(\omega)\\
&\hat{f}(\nu)\hat{g}(\nu)\\
&f(x)g(x)\FT{}\frac{1}{\sqrt{2\pi}}(\hat{f}*\hat{g})(\omega)\\
&\FT{}\frac{1}{2\pi}(\hat{f}*\hat{g})(\nu)
\end{align*}

\end{column}
\end{columns}
\end{wordonframe}

\begin{wordonframe}{Trasformata di Fourier notevoli}
\begin{align*}
& 1 & \sqrt{2\pi}\delta(\omega) & 2\pi\delta(\nu)\delta(x)\\
& \delta(x) & \frac{1}{\sqrt{2\pi}} & 1\\
& \exp{iax} & \sqrt{2\pi}\delta(\omega-a) & 2\pi\delta(\nu-a)\\
& x^n & i^n\sqrt{2\pi}\delta^{(n)}(\omega) & 2\pi i^n\delta^{(n)}(\nu)\\
& \frac{1}{x^n} & -i\sqrt{\frac{\pi}{2}}\frac{(-i\omega)^{n-1}}{(n-1)!}\sgn{(\omega)} & -i\pi\frac{(-i\nu)^{n-1}}{(n-1)!}\sgn{(\nu)}\\
& |x|^{\alpha} & \frac{-2}{\sqrt{2\pi}}\frac{\sin{(\frac{\pi\alpha}{2})}\Gamma(\alpha+1)}{|\omega|^{\alpha+1}} & -2\frac{\sin{(\frac{\pi\alpha}{2})}\Gamma(\alpha+1)}{|\nu|^{\alpha+1}}\\
& \frac{1}{\sqrt{|x|}} & \frac{1}{\sqrt{|\omega|}} & \sqrt{\frac{2\pi}{\nu}}\\
& \sgn{x} & \sqrt{\frac{2}{\pi}}\frac{1}{i\omega} & \frac{2}{i\nu}\\
& \rect{(ax)} & \frac{1}{\sqrt{2\pi a^2}}\sinc{(\frac{\omega}{2\pi a})} &  \frac{1}{|a|}\sinc{(\frac{\nu}{2\pi a})}\sinc{(ax)}\\
\end{align*}

\end{wordonframe}

\begin{wordonframe}{Trasformata di Fourier notevoli II}

\begin{align*}
&\sinc{(ax)} & \frac{1}{\sqrt{2\pi a^2}}\rect{(\frac{\omega}{2\pi a})} &  \frac{1}{|a|}\rect{(\frac{\nu}{2\pi a})}\\
&\sinc^2{(ax)} & \frac{1}{\sqrt{2\pi a^2}}\tri{(\frac{\omega}{2\pi a})} &  \frac{1}{|a|}\tri{(\frac{\nu}{2\pi a})}\\
&\tri{(ax)} & \frac{1}{\sqrt{2\pi a^2}}\sinc^2{(\frac{\omega}{2\pi a})} &  \frac{1}{|a|}\sinc^2{(\frac{\nu}{2\pi a})}\\
&\exp{-\alpha x^2} & \frac{1}{\sqrt{2\alpha}}\exp{-\frac{\omega^2}{4\alpha}} & \sqrt{\frac{\pi}{\alpha}}\exp{-\frac{\nu^2}{4\alpha}}\\
&\exp{-a|x|} & \sqrt{\frac{2}{\pi}}\frac{a}{a^2+\omega^2} & \frac{2a}{a^2+\nu^2}\exp{-\frac{a^2x^2}{2}}\\
&\exp{-\frac{a^2x^2}{2}}H_n(ax) & \frac{(-i)^n}{a}\exp{-\frac{\omega^2}{2a^2}}H_n(\frac{\omega}{a}) & \frac{(-i)^n\sqrt{2\pi}}{a}\exp{-\frac{\omega^2}{2a^2}}H_n(\frac{\nu}{a})
\end{align*}
\end{wordonframe}

\subsection{Legge grandi numeri}

\begin{frame}{Legge grandi numeri}\linkdest{LLN}
Variabile casuale ($X$) campionata n volte ($x_i$) = Campione di dimensione n.
    Media aritmetica di n variabili $X_i$: $\E{\overline{X}_n}=\mu$; Finite variance: $\var{X_i}=\sigma^2$: $\prob{(|\overline{X}_n-\mu|>\epsilon)}\leq\frac{\sigma^2}{n\epsilon^2}$
    \begin{columns}
    \begin{column}{0.5\textwidth}
\begin{align*}
&\var{\overline{X}_n}=\var{\frac{1}{n}(X_1+\ldots+X_n)}\\
&=\frac{1}{n^2}\var{(X_1+\ldots+X_n)}=\frac{n\sigma^2}{n^2}=\frac{\sigma^2}{n}\\
&\prob{(|\overline{X}_n-\mu|>\epsilon)}\leq\frac{\sigma^2}{n\epsilon^2}
\end{align*}
\keyword{Chebyshev ineq}: $\prob{(|X-\mu|\geq k\sigma)}\leq\frac{1}{k^2}$.
\keyword{Probability convergence}: Una sequenza di RV $\{X_n\}\to X$ in probabilit\'a se $\lim_{n\to\infty}\prob{(|X_n-X|)>\epsilon}$=0.
    \end{column}
    \begin{column}{0.5\textwidth}
Per ogni RV con $\mu$ finita $\phi_X(t)\approx1+i\mu t+o(t)$,$\phi_{\frac{1}{n}X}(t)=\phi_X(\frac{t}{n})$, $\phi_{X+Y}(t)=\phi_X(t)\phi_Y(t)$:
$\phi_{\overline{X}_n}(t)=[\phi_X(t)]^n=[1+it\mu+\ldots]^n\to\exp{i\mu t}$; Levy theorem: $\overline{X}_n\to\mu$.
    \end{column}
    \end{columns}
\end{frame}

\begin{frame}{LLN: asintoticamente gaussiana}
Se esistono $\mu_1, \mu_2$ finiti vale LLN: $\overline{x}_n-\mu\xrightarrow{P}0$.
\begin{columns}[T]
\begin{column}{0.5\textwidth}
\begin{align*}
&\var{(\overline{x}_n-\mu)}=\var{(\frac{S_N}{N})}\\
&=\frac{1}{N^2}\var{(S_N)}=\frac{\sigma^2}{N}\\
&\var{(S_N)}=\sum(\PDy{x_i}{S_N})^2\sigma_i^2\\
&\phi(t)=\phi_{X-\mu}
\end{align*}
\end{column}
\begin{column}{0.5\textwidth}
\begin{align*}
&y_N=\sqrt{\frac{N}{\sigma^2}}(\overline{x}-\mu)\\
&\E{[y_N]}=0,\ \var{[y_N]}=1\\
&\phi_N=[\phi(\frac{t}{\sigma\sqrt{N}})]^N
\end{align*}
\end{column}
\end{columns}
\begin{align*}
&\log{\phi_N}=N\log{\phi(\frac{t}{\sigma\sqrt{N}})}\approx N[\frac{it\mu}{\sigma\sqrt{N}}+\frac{(it)^2\sigma^2}{2\sigma N}+o(\frac{t^3}{N\expy{\frac{3}{2}}})]\\
&\log{\phi_N}\to-\frac{t^2}{2}\ \text{Per T Levy la pdf \'e:}\\
&\frac{1}{2\pi}\int\exp{-\frac{t^2}{2}}\exp{-itx}d\,t=\frac{1}{2\pi}\int\exp{(t+ix)^2-\frac{x^2}{2}}d\,t=\frac{1\exp{-\frac{x^2}{2}}}{\sqrt{2\pi}}
\end{align*}
\end{frame}

\begin{wordonframe}{Espansioni di Taylor}
\begin{align*}
&\phi(t)=1+it\mu-\frac{\sigma^2t^2}{2}+o(t^2)\\
&\log{(1+x)}\approx x-\frac{x^2}{2}+o(x^2)
\end{align*}
\end{wordonframe}